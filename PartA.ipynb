{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "70MQAdsq-W8h",
        "outputId": "2844beaf-710e-48eb-890f-749a4a87b8cc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pytorch_lightning'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-05c5833d3bbc>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mFunn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_lightning'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import statistics\n",
        "from torch import nn\n",
        "from torch.nn import functional as Funn\n",
        "from torchvision import datasets, transforms\n",
        "import pytorch_lightning as pl\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset ,DataLoader\n",
        "from torchmetrics import Accuracy\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as Fnn\n",
        "from torch.utils.data import DataLoader, sampler, random_split\n",
        "from torchvision import models\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import wandb"
      ],
      "metadata": {
        "id": "Ks9ZkFTg-fPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login(key=\"ed57c3903aa24b40dc30a68b77aad62d1489535b\")\n",
        "pName = \"CS6910 - Assignment 2\"\n",
        "run_obj=wandb.init( project=pName)"
      ],
      "metadata": {
        "id": "2-qiqk7D-lV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('-wp','--wandb_project',default ='myprojectname',metavar=\"\",required = False,type=str,help = \"Project name used to track experiments in Weights & Biases dashboard\" )\n",
        "parser.add_argument('-we','--wandb_entity',default ='myname',metavar=\"\",required = False,type=str,help = \"Wandb Entity used to track experiments in the Weights & Biases dashboard.\" )\n",
        "parser.add_argument('-e','--epochs',default=10,metavar=\"\",required = False,type=int,help = \"Number of epochs to train neural network.\")\n",
        "parser.add_argument('-do','--drop_out',default=0.3,metavar=\"\",required = False,type=float,help = \"Dropout\")\n",
        "parser.add_argument('-lr','--learning_rate',default=0.0001,metavar=\"\",required = False,type=float,help = \"Learning rate used to optimize model parameters\")\n",
        "parser.add_argument('-a','--activation_function',default='GELU',metavar=\"\",required = False, help = \"Activation Function\", type=str,choices= [\"SiLU\", \"Mish\", \"GELU\", \"ReLU\"])\n",
        "parser.add_argument('-bn','--batch_normalization',default='No',metavar=\"\",required = False,type=str, help = \"batch normalization\", choices= [\"Yes\", \"No\"])\n",
        "parser.add_argument('-da','--data_augmentation',default='No',metavar=\"\",required = False, type=str,help = \"data augmentation\", choices= [\"Yes\", \"No\"])\n",
        "parser.add_argument('-fz','--filter_size',default=64,metavar=\"\",required = False, type=int,help = \"filter_size\")\n",
        "parser.add_argument('-fo','--filter_organisation',default=\"same\",metavar=\"\",required = False,type=str, help = \"filter_organisation\", choices= [\"same\",\"half\",\"double\"])\n",
        "\n",
        "args=parser.parse_args()\n",
        "\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256,256)),\n",
        "\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_augmented = transforms.Compose([\n",
        "    transforms.Resize((256,256)),\n",
        "    transforms.AutoAugment(),\n",
        "\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "# Load dataset from directory\n",
        "if(args.data_AUG==\"No\"):\n",
        "    dataset = datasets.ImageFolder('inaturalist_12K/train', transform=transform)\n",
        "else:\n",
        "    dataset = datasets.ImageFolder('/content/inaturalist_12K/train', transform=transform_augmented)\n",
        "\n",
        "test_dataset = datasets.ImageFolder('/content/inaturalist_12K/val', transform=transform)\n",
        "#train_dataset = datasets.ImageFolder('/content/inaturalist_12K/train', transform=transform)\n",
        "#test_dataset = datasets.ImageFolder('/content/inaturalist_12K/val', transform=transform)\n",
        "\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [int(0.8*len(train_dataset)), len(train_dataset)-int(0.8*len(train_dataset))])\n",
        "\n",
        "# Create data loader objects for training and testing sets\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "#test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "x1McVIaJ-wdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_Train(pl.LightningModule):\n",
        "\n",
        "  def __init__(self,activation_FUN,batch_NORM,data_AUG,filter_ORG,drop_OUT):\n",
        "\n",
        "\n",
        "    self.activation_FUN=activation_FUN\n",
        "    self.batch_NORM=batch_NORM\n",
        "\n",
        "\n",
        "    #syntax:The syntax is torch.nn.Conv2d(in_channels, out_channels, kernel_size)\n",
        "\n",
        "    super(CNN_Train,self).__init__()\n",
        "\n",
        "    self.convo_1 = torch.nn.Conv2d(3,filter_ORG[0],3)\n",
        "    self.convo_2 = torch.nn.Conv2d(filter_ORG[0], filter_ORG[1], 3)\n",
        "    self.convo_3 = torch.nn.Conv2d(filter_ORG[1], filter_ORG[2], 3)\n",
        "    self.convo_4 = torch.nn.Conv2d(filter_ORG[2], filter_ORG[3], 3)\n",
        "    self.convo_5 = torch.nn.Conv2d(filter_ORG[3], filter_ORG[4], 3)\n",
        "\n",
        "    if(activation_FUN==\"ReLU\"):\n",
        "              self.activation_FUN=nn.ReLU()\n",
        "    elif(activation_FUN==\"GELU\"):\n",
        "             self.activation_FUN=nn.GELU()\n",
        "    elif(activation_FUN==\"SiLU\"):\n",
        "              self.activation_FUN=nn.SiLU()\n",
        "    elif(activation_FUN==\"Mish\"):\n",
        "             self.activation_FUN=nn.Mish()\n",
        "    else:\n",
        "             print(\"ERROR\")\n",
        "\n",
        "    stride=2\n",
        "    input_dimension=256\n",
        "\n",
        "    DenseLayerDimension = input_dimension\n",
        "    for filter in filter_ORG:\n",
        "      DenseLayerDimension = (DenseLayerDimension-4)//stride + 1\n",
        "\n",
        "\n",
        "    self.f_batch = nn.BatchNorm1d(DenseLayerDimension*DenseLayerDimension*filter_ORG[4])\n",
        "    self.maxpool= nn.MaxPool2d(2)\n",
        "    self.flatten= nn.Flatten()\n",
        "    self.fc_Layer= nn.Linear(DenseLayerDimension*DenseLayerDimension*filter_ORG[4],10)\n",
        "    #self.fc_Layer = nn.Linear(filter_ORG[4] * input_dimension * input_dimension, 10)\n",
        "    self.softmax= nn.Softmax()\n",
        "    self.learning_rate=0.001\n",
        "    self.s_dropout= nn.Dropout(p=drop_OUT)\n",
        "    self.save_hyperparameters()\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    output = self.activation_FUN(self.convo_1(x))\n",
        "    output = self.maxpool(output)\n",
        "\n",
        "    output = self.activation_FUN(self.convo_2(output))  # Apply activation after the convolution\n",
        "    output = self.maxpool(output)\n",
        "\n",
        "    output = self.activation_FUN(self.convo_3(output))  # Apply activation after the convolution\n",
        "    output = self.maxpool(output)\n",
        "\n",
        "    output = self.activation_FUN(self.convo_4(output))  # Apply activation after the convolution\n",
        "    output = self.maxpool(output)\n",
        "\n",
        "    output = self.activation_FUN(self.convo_5(output))  # Apply activation after the convolution\n",
        "    output = self.maxpool(output)\n",
        "\n",
        "    output = self.flatten(output)\n",
        "\n",
        "    if self.batch_NORM == \"Yes\":\n",
        "        output = self.f_batch(output)\n",
        "\n",
        "    output = self.s_dropout(output)\n",
        "\n",
        "    output = self.activation_FUN(self.fc_Layer(output))  # Apply activation to the fully connected layer\n",
        "    return output\n",
        "\n",
        "  def training_step(self, batch, batch_index):\n",
        "    x, y = batch\n",
        "    un_logits = self(x)\n",
        "    loss = Funn.cross_entropy(un_logits,y)\n",
        "    self.train_loss.append(loss)\n",
        "    accuracy = (un_logits.argmax(dim=1) == y).float().mean()\n",
        "    self.train_accuracy.append(accuracy)\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def validation_step(self, batch, batch_index):\n",
        "    x, y = batch\n",
        "    un_logits = self(x)\n",
        "    loss = Funn.cross_entropy(un_logits,y)\n",
        "    self.val_loss.append(loss)\n",
        "    accuracy = (un_logits.argmax(dim=1) == y).float().mean()\n",
        "    self.val_accuracy.append(accuracy)\n",
        "\n",
        "\n",
        "  def test_step(self, batch, batch_index):\n",
        "    x, y = batch\n",
        "    un_logits = self(x)\n",
        "    loss = Funn.cross_entropy(un_logits,y)\n",
        "    self.log(\"test loss\",loss, prog_bar=True)\n",
        "    accuracy = (un_logits.argmax(dim=1) == y).float().mean()\n",
        "    self.log(\"test accuracy\",accuracy,prog_bar=True)\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "    return optimizer\n",
        "\n",
        "    filter_size=64\n",
        "    if(arg.filter_ORG==\"same\"):\n",
        "      filter_organisation1 = [arg.filter_size]*5\n",
        "    elif(arg.filter_ORG==\"half\"):\n",
        "      filter_organisation1=[arg.filter_size,arg.filter_size//2,arg.filter_size//4,arg.filter_size//8,arg.filter_size//16]\n",
        "    elif(arg.filter_ORG==\"double\"):\n",
        "      filter_organisation1=[arg.filter_size,arg.filter_size*2,arg.filter_size*4,arg.filter_size*8,arg.filter_size*16]\n",
        "\n",
        "    obj = CNN_Train(arg.activation_FUN,arg.batch_NORM,arg.data_AUG,arg.filter_ORG,arg.drop_OUT)\n",
        "\n",
        "    trainer = pl.Trainer(max_epochs=arg.epochs) #, accelerator=\"gpu\", devices=1)\n",
        "\n",
        "    trainer.fit(model=obj,train_dataloaders=train_dataloader,val_dataloaders=val_dataloader)\n",
        "\n",
        "\n",
        "wandb.finish()\n",
        "\n",
        "    wandb.log({\"train loss\":train_loss,\"train accuracy\":train_accuracy,\"val loss\":val_loss,\"val accuracy\":val_accuracy})"
      ],
      "metadata": {
        "id": "21q0fxU-AQo0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}